{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aa368d9-75a8-4b9f-96e7-56361b91faa6",
   "metadata": {},
   "source": [
    "### AI  > ML > Deep Learning > Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e63f11-c810-43d5-97f3-c4e0b8a7f786",
   "metadata": {},
   "source": [
    "# ANN\n",
    "## Str of Neural Network (ANN)\n",
    "### 3 Layers =>\n",
    "- Input Layer : Data inputed to NN; Each Circle represents one feature (a piece of info); Circles mean's variables / data set fed into it\n",
    "- Hidden layer 1 & 2 : processing\n",
    "- Output layer : Result\n",
    "* Perceptron : No Hidden layer; Direct from input to Output ; Linear classifier; has many limitations \n",
    " \n",
    "#### Comparison with Neuron :-\n",
    "* Dendrite : refers to input \n",
    "* Axon : output delivered thru axon\n",
    "    - width of dendrites define the weight associated with it\n",
    " \n",
    "### ANN\n",
    "- X1, X2 .... <= Input Layer\n",
    "- Y1, Y2, ... <= Output Layer\n",
    "-  S =  Î£ X<sub>i</sub> W<sub>i</sub>\n",
    "    - combinaions of X and W based on path for each prticular layer\n",
    "- Hidden Layer : H1, H2, ...\n",
    "- Wi <==> Weights  \n",
    "- Bias  : less than 1; B1, B2, ...\n",
    "- Activation with sigmoid fn => The output for a single layer (process of forward propagation)\n",
    "- Z ==> S + bias\n",
    "- Sigmoid Fn : $$ x = {\\\\  1 \\over 1 + e^{-z} } $$ can also be taken as (1- e * x)\n",
    "- Compare resulatant of ouput with Original value (given); if they are same, procedure can be stopped; if not we apply baclkward propation for updating weights\n",
    "- Backward Propagation : resumes where we left off, ie; backward (from right-to-left)\n",
    "    - New val of wi, $$ wi =  Old_Val_of_wi - [(eta_symbol) * derivative of { Error \\over old_Val_of_wi } ]$$\n",
    "    - ie; ivide, eta vechu all related weights for each output nokkanam/hidden layer \n",
    "        ie; differential of error for each dependant of Y<sub>i</sub> and/or H<sub>i</sub>\n",
    "     - (eta) is,  learning rate  ; decimals less than 1\n",
    "     - Chain rule of partial derivatives\n",
    "     * Str reaches left End of first cycle of this process : End of first iteration\n",
    "         1. Apply the same forward propagation;\n",
    "         2. Check values\n",
    "         3. If (!notEqual); \n",
    "            - Apply backward Propagation\n",
    "         4. Else \n",
    "             - Procedure terminates when it's done. \n",
    "- Gradient--- procedure => uses derivations/ differential calc\n",
    "- Gradient Descent\n",
    "    - Error is taken by mean_quare_error\n",
    "    - \n",
    "    - if this get's too complex, vanishing gradient will occur; procedures like LSDM is used\n",
    "- Manual process takes more than an 1hr\n",
    "- Steps :\n",
    "    1. Get Inputs X1, X2, X3 ...\n",
    "    2. Compute S, add bias to get Z\n",
    "    3. Apply activation with sigmoid fn; marks end of forward propagation; to get resultant output\n",
    "    4. Loop begins\n",
    "    5. Updation of Weights : Checking as mentioned above\n",
    "    - if found dissimilar; apply Backward Propagation until target value == output value\n",
    "    6. Terminate Process\n",
    "\n",
    "### Note : \n",
    "- RBM & Autoencoders\n",
    "- DNN is ANN with multiple hidden layers\n",
    "- CNN for image\n",
    "- RNN for sequential data\n",
    "\n",
    "    - Problems : Vanishing gradient, exploding gradient; Soln's like : LSDM\n",
    "    - LSDM, GRU, Gated RNN, Noturn encoders\n",
    "    \n",
    "- Multilayer Perceptron == Deep Feedforward network === feedforward neural network\n",
    "- z == linear regression; activation formular from logistical\n",
    "- ANN - speech recognition, anomaly detection, audio generation, spell check\n",
    "- Activation fn s :\n",
    " - Linear : fn is same as z\n",
    " - Step : if z less than 0, 0; greater than or equal to 0 ; is; like step fn in mathematics\n",
    " - Sigmpid : as above\n",
    " - Softmax : extension of sigmoid; e^x / summ of e^x; CNN; multiple inputs\n",
    " - ReLU : ; used for CNN\n",
    " - Hyperbolic : tanh\n",
    " $$ tanh = \\frac{\\\\ e^x - e^-x}{e^x + e^-x} $$\n",
    " - loss fn - cross entropy(classification), log loss, hinge loss, exponential loss, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466951ce-7be0-4fe9-b456-2ad0052f19d1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c9612e9-b004-43f7-9b6a-f1802569debd",
   "metadata": {},
   "source": [
    "### Practicals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512e35b1-abb8-4f1d-990b-fca81d7751f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
