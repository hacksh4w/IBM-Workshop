{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd333dd7-103b-462d-8520-ac79a07630f2",
   "metadata": {},
   "source": [
    "<center><h1>Deep Learning</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec0e3ae-9919-44cd-ba96-ea3148ce74a0",
   "metadata": {},
   "source": [
    "- Single layer perceptron, no hidden layer, Linear classifier\n",
    "- There are many disadvantages to Single layer perceptron so we use Multi Layer Perceptron, as it consists atleast one hidden layer\n",
    "- CNN is used for images \n",
    "- RNN is used for sequential data\n",
    "- RBM: Restricted Boltzmann Machine?\n",
    "- Deep Neural Networks are used for classification type models \n",
    "\n",
    "<h2>Artificial Neural Networks</h2>\n",
    "\n",
    "- x1,x2,x3,....,xn are inputs\n",
    "- w1,w2,w3,....,wn are weights\n",
    "- weights are different amounts of effect each type of input has on the input\n",
    "- Bias is calculated by S = Î£XiWi same as Linear Regression\n",
    "- Check out principle of least squares\n",
    "- Activation of bias is done by using sigmoid function \n",
    "- Summation => Addition of bias if any => Sigmoid function => output is the output of the layer\n",
    "- After getting the output of activation or Transfer function, We again apply the weights and appropriate bias and then sigmoid function to get the output of Forward propagation y1 and y2\n",
    "- Next step is backward propagation \n",
    "    1) Compare the output to original given values \n",
    "    2) If it is same, we can stop at this step\n",
    "    3) If it is not, then we use the output to update the weights which can take upto 1 hour\n",
    "    4) To update the weights, we use something called gradient descent, similar to slope\n",
    "    5) During backward propagation, our aim is to update the values of each of the weights\n",
    "    6) If there is any change in the weights, we apply forward propagation again\n",
    "    7) Steps are repeated\n",
    "- Gradient Desent\n",
    "    - We take total error by mean square method value summation \n",
    "    - We take Derivative of (Etotal / W5) to get the slope of that, to apply in the formula:\n",
    "        - New Weight = Old Weight - Assumed eeta value * Slope (for example Etotal/W5)\n",
    "    - Etotal/W5, applying chain rule as W5 is dependent on output of output of y2, y2(summation) and hence H1\n",
    "    - Same process is extended till we get input \n",
    "        - Etotal/w1 = e1/out h1 + e2/out h2\n",
    "        - e1 depends on y1, e2 depends on h2\n",
    "        - y1 depends on y output (backward equation)\n",
    "    - Then, as continued we apply forward propagation and repeat the cycle till we get the correct output\n",
    "- Each Hidden layer, we apply the same process again and again until the output is similar to correct value.\n",
    "- Look out for Artificial Neural Network Problems in youtube\n",
    "- Vanishing Gradient:\n",
    "    - As we do multiple iterations, gradient could get reduced to 0\n",
    "- Exploding Gradient: \n",
    "    - As we do multiple iterations, gradient could explode into higher values\n",
    "- We have to use multiple methods to get control these effects\n",
    "- Activiation function in Artificial Neural Networks is sigmoid function \n",
    "\n",
    "<h3> Activation Functions </h3>\n",
    "\n",
    "- Linear function / Identity function \n",
    "    - Backward propagation not possible\n",
    "- Step Function \n",
    "- Sigmoid function \n",
    "- Hyperbolic Tangent Function \n",
    "- Rectified Linear Unit Function (ReLu)\n",
    "    - max(0,x), less than 0 means 0, more than x means x itself \n",
    "- Softmax Activation Function \n",
    "    - Can only be used when there are multiple inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a497f3a-7526-4762-ad24-c691f4d5d4ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1]\n",
      " [1 1 1]\n",
      " [0 1 0]\n",
      " [0 1 1]]\n",
      "[[1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "input_array = np.array([[1,0,1],[1,1,1],[0,1,0],[0,1,1]])\n",
    "output_array = np.array([[1],[1],[0],[0]])\n",
    "print(input_array)\n",
    "print(output_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb4d3bc5-6a80-4f59-9518-24ef4604fb81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.77132064]\n",
      " [0.02075195]\n",
      " [0.63364823]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "weights = np.random.random([3,1])\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "323a5ff7-0345-4a78-b027-7b0f1bf8d4a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8061162 ]\n",
      " [0.80933899]\n",
      " [0.51018658]\n",
      " [0.66248773]]\n",
      "[[ 0.1938838 ]\n",
      " [ 0.19066101]\n",
      " [-0.51018658]\n",
      " [-0.66248773]]\n"
     ]
    }
   ],
   "source": [
    "sum_bias = 0.02 + np.dot(input_array,weights)\n",
    "def activation(x):\n",
    "    return 1/(1+ np.exp(-x))\n",
    "predicted_output = activation(sum_bias)\n",
    "print(predicted_output)\n",
    "error = output_array - predicted_output\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572d8e34-fa3a-44d5-b842-448a69b455dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
